
overfitting training set
nth power of polynomial
d = degree of polynomial
linear equation, square equation, cubic equation
5th order polynomial
generalize hypothesis
training set
cv set
test set
data to fit parameters might work better on model
    than other data sets
model selection
data subset (75/25, 70/30, 60/20/20)
cross validation
60% training set
20% cross validation set
20% test set
run cross validation set through nth power model equations
cross validation data selects the model
diagnosing bias vs. variance
high bias underfit data
high variance overfit data
goodness of fit
learning curves
clustering
    k-means (k clusters)
    agglomerative clustering, hierarchy of clusters
k-folds
k-means, k number of clusters
linear regression
    y = mx + b
    coefficients weights
linear algebra
    matrices and vectors
linear regression
logistic regression
    sigmoid function
regularization
    choose the regularization value lambda
        to get an appropriate fit
neural networks
machine learning system design
support vector machines
    hyperplane
unsupervised learning
    clustering
    k-means
dimensionality reduction
anomaly detection
recommender systems
large scale machine learning

towardsdatascience.com
    design patterns
        layers
        mvc, model-view-controller

standard deviation
    find vector mean value
    for each vector value subtract the mean and square the result
    find the mean value of the squared differences
    take square root of the squared diff mean value
    
//////////////////////////////////////////////////////////////////////////////
//
//////////////////////////////////////////////////////////////////////////////

Coursera Machine Learning Syllabus

Intro

Video: What is Machine Learning?
Reading: What is Machine Learning?
Video: Supervised Learning
Reading: Supervised Learning
Video: Unsupervised Learning
Reading: Unsupervised Learning


Linear Regression with One Variable

Linear regression predicts a real-valued output based on an input value. We discuss the application of linear regression to housing price prediction, present the notion of a cost function, and introduce the gradient descent method for learning.

    Video: Model Representation
    Reading: Model Representation
    Video: Cost Function
    Reading: Cost Function
    Video: Cost Function - Intuition I
    Reading: Cost Function - Intuition I
    Video: Cost Function - Intuition II
    Reading: Cost Function - Intuition II
    Video: Gradient Descent
    Reading: Gradient Descent
    Video: Gradient Descent Intuition
    Reading: Gradient Descent Intuition
    Video: Gradient Descent For Linear Regression
    Reading: Gradient Descent For Linear Regression
    Reading: Lecture Slides


Linear Algebra Review
This optional module provides a refresher on linear algebra concepts. Basic understanding of linear algebra is necessary for the rest of the course, especially as we begin to cover models with multiple variables.

    Video: Matrices and Vectors
    Reading: Matrices and Vectors
    Video: Addition and Scalar Multiplication
    Reading: Addition and Scalar Multiplication
    Video: Matrix Vector Multiplication
    Reading: Matrix Vector Multiplication
    Video: Matrix Matrix Multiplication
    Reading: Matrix Matrix Multiplication
    Video: Matrix Multiplication Properties
    Reading: Matrix Multiplication Properties
    Video: Inverse and Transpose
    Reading: Inverse and Transpose
    Reading: Lecture Slides
    Practice Quiz: Linear Algebra

Linear Regression with Multiple Variables
What if your input has more than one value? In this module, we show how linear regression can be extended to accommodate multiple input features. We also discuss best practices for implementing linear regression.
8 videos, 16 readings

    Reading: Setting Up Your Programming Assignment Environment
    Reading: Accessing MATLAB Online and Uploading the Exercise Files
    Reading: Installing Octave on Windows
    Reading: Installing Octave on Mac OS X (10.10 Yosemite and 10.9 Mavericks and Later)
    Reading: Installing Octave on Mac OS X (10.8 Mountain Lion and Earlier)
    Reading: Installing Octave on GNU/Linux
    Reading: More Octave/MATLAB resources
    Video: Multiple Features
    Reading: Multiple Features
    Video: Gradient Descent for Multiple Variables
    Reading: Gradient Descent For Multiple Variables
    Video: Gradient Descent in Practice I - Feature Scaling
    Reading: Gradient Descent in Practice I - Feature Scaling
    Video: Gradient Descent in Practice II - Learning Rate
    Reading: Gradient Descent in Practice II - Learning Rate
    Video: Features and Polynomial Regression
    Reading: Features and Polynomial Regression
    Video: Normal Equation
    Reading: Normal Equation
    Video: Normal Equation Noninvertibility
    Reading: Normal Equation Noninvertibility
    Video: Working on and Submitting Programming Assignments
    Reading: Programming tips from Mentors
    Reading: Lecture Slides

Octave/Matlab Tutorial
This course includes programming assignments designed to help you understand how to implement the learning algorithms in practice. To complete the programming assignments, you will need to use Octave or MATLAB. This module introduces Octave/Matlab and shows you how to submit an assignment.
6 videos, 1 reading

    Video: Basic Operations
    Video: Moving Data Around
    Video: Computing on Data
    Video: Plotting Data
    Video: Control Statements: for, while, if statement
    Video: Vectorization
    Reading: Lecture Slides
    Programming: Linear Regression

Logistic Regression
Logistic regression is a method for classifying data into discrete outcomes. For example, we might use logistic regression to classify an email as spam or not spam. In this module, we introduce the notion of classification, the cost function for logistic regression, and the application of logistic regression to multi-class classification.
7 videos, 8 readings

    Video: Classification
    Reading: Classification
    Video: Hypothesis Representation
    Reading: Hypothesis Representation
    Video: Decision Boundary
    Reading: Decision Boundary
    Video: Cost Function
    Reading: Cost Function
    Video: Simplified Cost Function and Gradient Descent
    Reading: Simplified Cost Function and Gradient Descent
    Video: Advanced Optimization
    Reading: Advanced Optimization
    Video: Multiclass Classification: One-vs-all
    Reading: Multiclass Classification: One-vs-all
    Reading: Lecture Slides

Regularization
Machine learning models need to generalize well to new examples that the model has not seen in practice. In this module, we introduce regularization, which helps prevent models from overfitting the training data.
4 videos, 5 readings

    Video: The Problem of Overfitting
    Reading: The Problem of Overfitting
    Video: Cost Function
    Reading: Cost Function
    Video: Regularized Linear Regression
    Reading: Regularized Linear Regression
    Video: Regularized Logistic Regression
    Reading: Regularized Logistic Regression
    Reading: Lecture Slides
    Programming: Logistic Regression

Neural Networks: Representation
Neural networks is a model inspired by how the brain works. It is widely used today in many applications: when your phone interprets and understand your voice commands, it is likely that a neural network is helping to understand your speech; when you cash a check, the machines that automatically read the digits also use neural networks.
7 videos, 6 readings

    Video: Non-linear Hypotheses
    Video: Neurons and the Brain
    Video: Model Representation I
    Reading: Model Representation I
    Video: Model Representation II
    Reading: Model Representation II
    Video: Examples and Intuitions I
    Reading: Examples and Intuitions I
    Video: Examples and Intuitions II
    Reading: Examples and Intuitions II
    Video: Multiclass Classification
    Reading: Multiclass Classification
    Reading: Lecture Slides
    Programming: Multi-class Classification and Neural Networks

Neural Networks: Learning
In this module, we introduce the backpropagation algorithm that is used to help learn parameters for a neural network. At the end of this module, you will be implementing your own neural network for digit recognition.
8 videos, 8 readings

    Video: Cost Function
    Reading: Cost Function
    Video: Backpropagation Algorithm
    Reading: Backpropagation Algorithm
    Video: Backpropagation Intuition
    Reading: Backpropagation Intuition
    Video: Implementation Note: Unrolling Parameters
    Reading: Implementation Note: Unrolling Parameters
    Video: Gradient Checking
    Reading: Gradient Checking
    Video: Random Initialization
    Reading: Random Initialization
    Video: Putting It Together
    Reading: Putting It Together
    Video: Autonomous Driving
    Reading: Lecture Slides
    Programming: Neural Network Learning

Advice for Applying Machine Learning
Applying machine learning in practice is not always straightforward. In this module, we share best practices for applying machine learning in practice, and discuss the best ways to evaluate performance of the learned models.
7 videos, 7 readings

    Video: Deciding What to Try Next
    Video: Evaluating a Hypothesis
    Reading: Evaluating a Hypothesis
    Video: Model Selection and Train/Validation/Test Sets
    Reading: Model Selection and Train/Validation/Test Sets
    Video: Diagnosing Bias vs. Variance
    Reading: Diagnosing Bias vs. Variance
    Video: Regularization and Bias/Variance
    Reading: Regularization and Bias/Variance
    Video: Learning Curves
    Reading: Learning Curves
    Video: Deciding What to Do Next Revisited
    Reading: Deciding What to do Next Revisited
    Reading: Lecture Slides
    Programming: Regularized Linear Regression and Bias/Variance

Machine Learning System Design
To optimize a machine learning algorithm, you’ll need to first understand where the biggest improvements can be made. In this module, we discuss how to understand the performance of a machine learning system with multiple parts, and also how to deal with skewed data.
5 videos, 3 readings

    Video: Prioritizing What to Work On
    Reading: Prioritizing What to Work On
    Video: Error Analysis
    Reading: Error Analysis
    Video: Error Metrics for Skewed Classes
    Video: Trading Off Precision and Recall
    Video: Data For Machine Learning
    Reading: Lecture Slides

Support Vector Machines
Support vector machines, or SVMs, is a machine learning algorithm for classification. We introduce the idea and intuitions behind SVMs and discuss how to use it in practice.
6 videos, 1 reading

    Video: Optimization Objective
    Video: Large Margin Intuition
    Video: Mathematics Behind Large Margin Classification
    Video: Kernels I
    Video: Kernels II
    Video: Using An SVM
    Reading: Lecture Slides
    Programming: Support Vector Machines

Unsupervised Learning
We use unsupervised learning to build models that help us understand our data better. We discuss the k-Means algorithm for clustering that enable us to learn groupings of unlabeled data points.
5 videos, 1 reading

    Video: Unsupervised Learning: Introduction
    Video: K-Means Algorithm
    Video: Optimization Objective
    Video: Random Initialization
    Video: Choosing the Number of Clusters
    Reading: Lecture Slides

Dimensionality Reduction
In this module, we introduce Principal Components Analysis, and show how it can be used for data compression to speed up learning algorithms as well as for visualizations of complex datasets.
7 videos, 1 reading

    Video: Motivation I: Data Compression
    Video: Motivation II: Visualization
    Video: Principal Component Analysis Problem Formulation
    Video: Principal Component Analysis Algorithm
    Video: Reconstruction from Compressed Representation
    Video: Choosing the Number of Principal Components
    Video: Advice for Applying PCA
    Reading: Lecture Slides
    Programming: K-Means Clustering and PCA

Anomaly Detection
Given a large number of data points, we may sometimes want to figure out which ones vary significantly from the average. For example, in manufacturing, we may want to detect defects or anomalies. We show how a dataset can be modeled using a Gaussian distribution, and how the model can be used for anomaly detection.
8 videos, 1 reading

    Video: Problem Motivation
    Video: Gaussian Distribution
    Video: Algorithm
    Video: Developing and Evaluating an Anomaly Detection System
    Video: Anomaly Detection vs. Supervised Learning
    Video: Choosing What Features to Use
    Video: Multivariate Gaussian Distribution
    Video: Anomaly Detection using the Multivariate Gaussian Distribution
    Reading: Lecture Slides

Recommender Systems
When you buy a product online, most websites automatically recommend other products that you may like. Recommender systems look at patterns of activities between different users and different products to produce these recommendations. In this module, we introduce recommender algorithms such as the collaborative filtering algorithm and low-rank matrix factorization.
Show less
6 videos, 1 reading

    Video: Problem Formulation
    Video: Content Based Recommendations
    Video: Collaborative Filtering
    Video: Collaborative Filtering Algorithm
    Video: Vectorization: Low Rank Matrix Factorization
    Video: Implementational Detail: Mean Normalization
    Reading: Lecture Slides
    Programming: Anomaly Detection and Recommender Systems

Large Scale Machine Learning
Machine learning works best when there is an abundance of data to leverage for training. In this module, we discuss how to apply the machine learning algorithms with large datasets.
6 videos, 1 reading

    Video: Learning With Large Datasets
    Video: Stochastic Gradient Descent
    Video: Mini-Batch Gradient Descent
    Video: Stochastic Gradient Descent Convergence
    Video: Online Learning
    Video: Map Reduce and Data Parallelism
    Reading: Lecture Slides

Application Example: Photo OCR
Identifying and recognizing objects, words, and digits in an image is a challenging task. We discuss how a pipeline can be built to tackle this problem and how to analyze and improve the performance of such a system.
5 videos, 1 reading

    Video: Problem Description and Pipeline
    Video: Sliding Windows
    Video: Getting Lots of Data and Artificial Data
    Video: Ceiling Analysis: What Part of the Pipeline to Work on Next
    Reading: Lecture Slides
    Video: Summary and Thank You





//////////////////////////////////////////////////////////////////////////////
//
//////////////////////////////////////////////////////////////////////////////

What is Machine Learning?

web search engines, data mining, click stream data, engineering

Arthur Samuel wrote checkers program
Tom Mitchell (1998), task T, experience E, performance P

supervised learning
unsupervised learning

machine learning is a set of tools, know how to use these tools

reduce correlated variables in vector input


Supervised Learning

example, predict housing prices in Portland, Oregon
some predictive model choices
    straight line through data
    quadratic formula through data
regression algorithm: predict continuous valued output
classification algorithm: predict classes, discrete valued output


Unsupervised Learning

data has no labels
clustering
market segmentation when segments need discovery

Linear Regression

m number of training examples
X input variable vector, features
y target output

>>> X = [[2104],[1416],[1534],[852]]
>>> y = [460,232,315,178]
>>> xy_zip_line = list(zip(X,y))
>>> xy_zip_line
[(2104, 460), (1416, 232), (1534, 315), (852, 178)]
>>> lr = LinearRegression()
>>> linreg_model = lr.fit(X,y)
>>> linreg_model.predict([[950]])
array([175.35527991])

training set ----> algorithm ----> target output

cost function
    minimize difference between model predicted value and actual value
    
1/2m sum (f(X_features) - y_target) ^ 2
    
>>> linreg_model.predict(X)
array([440.33629982, 282.35801412, 309.45312707, 152.85255899])
>>> y
[460, 232, 315, 178]
>>> model_p = [440.34, 282.36, 309.45, 152.85]
>>> diff = [round((model_p[idx] - y[idx]),2) for idx in range(len(model_p))]
>>> diff
[-19.66, 50.36, -5.55, -25.15]
>>> diff_squared = [ round((diff[idx]**2),2) for idx in range(len(model_p)) ]
>>> diff_squared
[386.52, 2536.13, 30.8, 632.52]

cost function in this example
    (1/2m)(440.34 - 460)^2 + (282.36 - 232)^2 + 
        (309.45 - 315)^2 + (152.85 - 178)^2
        
    (1/(2*4))(386.52 + 2536.13 + 30.8 + 632.52)
    
    448.25
    
gradient descent
    optimization algorithm for finding minimum of a function
    used to tune parameters
    
    find minimum for parameters using alpha value (learning rate)
        and derivative of cost function
        
    algorithm
        param = param - (alpha * function derivative)
        
derivative slope of line tangent to a point

if alpha is too small, gradient descent can be slow

if alpha is too large, gradient descent can overshoot minimum

linear regression fits a straight line to the data

batch gradient descent 
    each step of batch gradient descent uses all the training examples
    
Linear Algebra Review

matrix rectangular array of numbers
matrix vector multiplication
    matrix rows equal vector size
    multiply each matrix row element by vector value of same index
        then add the values together
        
num_rows = np.size(np_mtx,0)
num_cols = np.size(np_mtx,1)

# matrix vector multiplication in python
>>> np_mtx
array([[ 1,  2,  1,  5],
       [ 0,  3,  0,  4],
       [-1, -2,  0,  0]])
>>> np_vec
array([1, 3, 2, 1])
>>> mtx_vector_result = []
>>> num_cols = np.size(np_mtx,1)
>>> for i in range(len(np_mtx)):
...     vals = []
...     for j in range(num_cols):
...         vals.append(np_mtx[i,j] * np_vec[j])
...     mtx_vector_result.append(sum(vals))
...
>>> mtx_vector_result
[14, 13, -7]

>>> def mtx_vector_mult(mtx, vec):
...     mtx_vector_result = []
...     num_cols = np.size(mtx,1)
...     for i in range(len(mtx)):
...         vals = []
...         for j in range(num_cols):
...             vals.append(mtx[i,j] * vec[j])
...         mtx_vector_result.append(sum(vals))
...     return mtx_vector_result
...

>>> house_size_mtx = np.array([[1,2104],[1,1416],[1,1534],[1,852]])
>>> house_size_vec = np.array([-40,.25])
>>> mtx_vector_mult(house_size_mtx, house_size_vec)
[486.0, 314.0, 343.5, 173.0]

# transpose matrix
>>> tasty
matrix([[1, 2, 0],
        [3, 5, 9]])
>>> tasty.transpose()
matrix([[1, 3],
        [2, 5],
        [0, 9]])
        
gradient descent helps to tune parameters
    uses alpha (learning rate) and partial derivatives
    
feature scaling
    
scale features to make gradient descent more efficient

mean normalization
    (value - mean of values) / (scale value)
    
>>> house_X_mtx
   Size  BRNum  FLNum  Age
1  2104      5      1   45
2  1416      3      2   40
3  1534      3      2   30
4   852      2      1   36
>>> house_sz_scale = 2000
>>> [ (x - np.mean(house_X_mtx['Size'])) / house_sz_scale for x in house_X_mtx['Size'] ]
[0.31375, -0.03025, 0.02875, -0.31225]

gradient descent minimize cost function
    should converge
    declare convergence by threshold
        (example, 10^-3)

if gradient descent is not working, try smaller alpha

too many features, remove some or use regularization

polynomial regression
    nth power equation
    
there are algorithms to choose optimal quadratic equations
    and other machine learning models

m = number of training examples
n = number of features
    
>>> house_X_mtx
   Size  BRNum  FLNum  Age
1  2104      5      1   45
2  1416      3      2   40
3  1534      3      2   30
4   852      2      1   36

>>> house_y_target
array([460, 232, 315, 178])

>>> np.dot( house_X_mtx.transpose(), house_X_mtx)
array([[9510932,   21074,    8856,  228012],
       [  21074,      47,      19,     507],
       [   8856,      19,      10,     221],
       [ 228012,     507,     221,    5821]], dtype=int64)

>>> np.linalg.inv(np.dot( house_X_mtx.transpose(), house_X_mtx) )
array([[ 6.43878340e-04, -2.86097524e-01, -1.23945771e-01,
         4.40327647e-03],
       [-2.86097524e-01,  1.27479879e+02,  5.51285590e+01,
        -1.98969961e+00],
       [-1.23945771e-01,  5.51285590e+01,  2.44892323e+01,
        -8.76340244e-01],
       [ 4.40327647e-03, -1.98969961e+00, -8.76340244e-01,
         3.42637046e-02]])

>>> np.dot( house_X_mtx.transpose(), house_y_target )
array([1931218,    4297,    1732,   45838], dtype=int64)


>>> np.dot( 
    np.linalg.inv(np.dot(house_X_mtx.transpose(), house_X_mtx)), 
    np.dot(house_X_mtx.transpose(), house_y_target) )
array([   1.27168807, -456.83557783, -233.22016912,    6.7059192 ])

gradient descent/normal equation

gradient descent
    need to choose alpha
    many iterations
    works well even when n (num of features) is large
    
normal equation
    no need to choose alpha
    don't need to iterate
    need to compute inverse of X transpose * X
    slow if n is very large
    O(n^3)
    
>>> def calc_normal_equation(features_mtx, target_vec):
...     """(X-transpose * X)^-1 * X-transpose * y"""
...     return np.dot( np.linalg.inv(np.dot(features_mtx.transpose(), features_mtx)), 
...         np.dot(features_mtx.transpose(), target_vec) )
...

Logistic Regression
    sigmoid function binary classifier
    
Logistic function
    g(z) = 1 / (1+e^-z)
    
g(z) == 0.5 at z = 0

decision boundary
    separates data into classes

non-linear decision boundaries
    sometimes take the form of the equation of circle or ellipse
    
fit parameters by minimizing cost function with gradient descent  

optimization algorithms
    gradient descent
    conjugate gradient
    BFGS
    L-BFGS
    
complex optimization algorithms
    no need to manually pick alpha
    often faster than gradient descent    

how to minimize cost function
    take the derivative of the function and set it equal to zero
    
calculate gradient descent using python

example function: (x+5)^2

cur_x = 3 # The algorithm starts at x=3
rate = 0.01 # Learning rate
precision = 0.000001 #This tells us when to stop the algorithm
previous_step_size = 1 #
max_iters = 10000 # maximum number of iterations
iters = 0 #iteration counter
df = lambda x: 2*(x+5) #Gradient of our function

while previous_step_size > precision and iters < max_iters:
    prev_x = cur_x #Store current x value in prev_x
    cur_x = cur_x - rate * df(prev_x) #Grad descent
    previous_step_size = abs(cur_x - prev_x) #Change in x
    iters = iters+1 #iteration count
    print("Iteration",iters,"\nX value is",cur_x) #Print iterations
    
print("The local minimum occurs at", cur_x)

def gradient_descent(x_start, alpha, precision, max_iters, deriv_func):
    """Performs gradient descent to discover the cost function local min
    
    Params:
    x_start - starting pos for x when performing gradient descent
    alpha - learning rate of gradient descent calc
    precision - how close do we want to get to local min before happy
    max_iters - round, round, baby round, round like a record baby
    deriv_func - derivative of the cost function, 
        part of gradient descent operation
    """
    local_min = -99999.999
    cur_x = x_start
    previous_step_size = 1
    iters = 0 # gradient descent iteration counter
    while previous_step_size > precision and iters < max_iters:
        prev_x = cur_x # store current x value in prev_x
        cur_x = cur_x - alpha * deriv_func(prev_x) # grad descent on parameter
        previous_step_size = abs(cur_x - prev_x) # change in x, moving to convergence
        iters = iters + 1 #iteration count
        print("Iteration",iters,"\nX value is",cur_x) # print iterations, primitive debugging
    local_min = round(cur_x, 0)
    print("The local minimum occurs at", local_min)
    return local_min

# example func is (x+5) ** 2
# using chain rule, derivative of func is 2 * (x+5)
# func params: starting pos, alpha (learning rate), precision, max iterations, deriv func
gradient_descent(3, 0.01, 0.000001, 10000, lambda x: 2*(x+5))

logistic regression
    can perform multiclass classification
    
email foldering
    work friends family hobby
    healthy cold flu
    sunny cloudy rain snow
    
logistic regression multiclass classification
    fit a model n-class times
    
Regularization

avoid underfitting or overfitting data
want data to be able to generalize and work well on independent data set

overfit high variance, underfit high bias

gradient descent/normal equation

may be using too many features

X vector (features)
theta vector (parameters)

try model selection algorithms

regularization can use small values for params
    less prone to overfitting
    
minimizing cost function helps improve model accuracy in estimating 
    relationship between X and y values
    
reduce variance without sacrificing any important data properties

regularization is a form of regression

shrinks coefficients to zero
    avoids overfitting

RSS = sum of observations, i to n (y-observation - coefficient0 -
    sum of parameters/weights, j to p coefficient-j * x-parameter-ij)^2

Ridge regression
    shrinkage qty
        lambda * sum of parameters/weights, j to p coefficient-j ^ 2
        
RSS + shrinkage qty

sum of observations, i to n (y-observation - coefficient0 -
    sum of parameters/weights, j to p coefficient-j * x-parameter-ij)^2 +
        lambda * sum of parameters/weights, j to p coefficient-j ^ 2
        
Neural Networks

avoid overfitting, large num of features and complex computations

computer vision
    computer sees image as a matrix of values
        representing the image
        
50x50 pixel image
    2500 pixels, n = 2500
    
if the matrix is based on rgb values,
    a 50x50 matrix could have 7500 values (50x50x3)
    
neurons have inputs (X features) and output
    neurons perform computation on the input
    
neural network computation can be represented as 
    a sigmoid (logistic) activation function
    
g(z) = 1/(1+e^-z)

input layer ----> layer 2 ----> layer 3 ----> layer 4 ----> output layer

forward propagation vectorized implementation
    X feature set
    a activation set
    z set
    
superscript of activation value is the layer number where the 
    activation value resides
    
x0, x1, x2, xn ----> a0, a1, a2, an ----> z (output)

each layer has its own parameters

non-linear analysis/classification

sigmoid function
    f(0) = 0.5
    f(-4.6) = 0.01
    f(4.6) = 0.99

Neural Networks were used to read postal codes on mail

computer vision neural network example
    multi-class: pedestrian, car, motorcycle, truck
    output vector will have 4 elements

>>> pedestrian_output = [
... 1,
... 0,
... 0,
... 0
... ]
>>> car_output = [
... 0,
... 1,
... 0,
... 0
... ]
>>> motorcycle_output = [
... 0,
... 0,
... 1,
... 0
... ]
>>> truck_output = [
... 0,
... 0,
... 0,
... 1
... ]
>>> vehicle_bits
{'pedestrian': 1, 'car': 2, 'motorcycle': 4, 'truck': 8}

Neural Network
    L number of layers
    K number of classes
    
input has a weight

activation function 
    sigmoid function
    
big commercial neural networks may have up to 500 million training samples



    

//////////////////////////////////////////////////////////////////////////////
//
//////////////////////////////////////////////////////////////////////////////


sklearn.preprocessing
    PolynomialFeatures
    StandardScaler


sklearn.linear_model
    DecisionTreeClassifier
    DecisionTreeRegressor
    LinearRegression
    LogisticRegression
    RandomForestClassifier
    RandomForestRegressor

sklearn.neighbors
    NearestNeighbors

sklearn.ensemble
    AdaBoostClassifier
    AdaBoostRegressor

sklearn.svm
    LinearSVC
    SVC

sklearn.cluster
    KMeans

Model predicts the target vector 

from sklearn import linear_model, datasets
logistic = linear_model.LogisticRegression()

GridSearchCV()
RandomizedSearchCV()

>>> from sklearn.linear_model import LinearRegression
>>> from sklearn.datasets import load_boston
>>> boston = load_boston()
>>> features = boston.data[:,0:2]
>>> target = boston.target
>>> # train a model that represents a linear relationship (y = mx + b) between the feature and target vector
...
>>> regression = LinearRegression()
>>> model = regression.fit(features, target)

>>> model.intercept_
22.46681692105723
>>> model.coef_
array([-0.34977589,  0.11642402])
>>> target[0:9]
array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5])

>>> house_values = model.predict(features)

>>> for val in house_values[0:9]:
...     print(val * 1000)
...
24560.238723708437
22457.264541581142
22457.271537098888
22455.494675592534
22442.66489605453
22456.37611082797
23891.23547622156
23871.55708481458
23848.230530905385

>>> for idx in range(0,9):
...     print(str(idx) + ": " + str(house_values[idx] * 1000) + ", " + str(target[idx] * 1000))
...
0: 24560.238723708437, 24000.0
1: 22457.264541581142, 21600.0
2: 22457.271537098888, 34700.0
3: 22455.494675592534, 33400.0
4: 22442.66489605453, 36200.0
5: 22456.37611082797, 28700.0
6: 23891.23547622156, 22900.0
7: 23871.55708481458, 27100.0
8: 23848.230530905385, 16500.0




>>> features.size
1012
>>> target.size
506


 
 
 
k-folds

dataset
    subset-1
    subset-2
    subset-3
    subset-4
    subset-5

training data: subset-2, subset-3, subset-4, subset-5
test data: subset-1

training data: subset-1, subset-3, subset-4, subset-5
test data: subset-2

training data: subset-1, subset-2, subset-4, subset-5
test data: subset-3

training data: subset-1, subset-2, subset-3, subset-5
test data: subset-4

training data: subset-1, subset-2, subset-3, subset-4
test data: subset-5

 
 
Cross-validation, sometimes called rotation estimation,[1][2][3] or out-of-sample testing is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set).[4] The goal of cross-validation is to test the model’s ability to predict new data that were not used in estimating it, in order to flag problems like overfitting[citation needed] and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).

One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model’s predictive performance.

 
 
training data
test data
observations - features and targets
k-fold cross-validation
    split data into k parts called folds
    each time process is run different k data part is used as test set

preprocess and train model
scikit-learn pipeline package does preprocessing and training model

Fitting a data model

Let’s start with what a model is. A model is a description of a system, usually expressed as an equation of some kind. Let’s say we have some data – measurements of variables x and y. We think that in the future, we’ll have measurements of more x-like values, and we’d like to be able to predict those ys.

It looks like you could draw a nice straight line through this cloud of points. That means that a linear model might be a good choice for this data. We’ve just done the first step in the model-fitting process: we’ve decided to use a line – a simple linear model.

The process of picking the correct line for this model is called “fitting”. There are different ways to do this – least squares is possibly the most familiar one. You could also use the “wiggle a ruler around on paper” method, or the “draw lines in Powerpoint” method. We’ll skip the details of that step because the internet describes least-squares fairly well.

That fitted line can be described with the equation y=mx+b. When we fit the model what we’re really doing is choosing the values for m and b – the slope and the intercept. The point of fitting the model is to find this equation – to find the values of m and b such that y=mx+b describes a line that fits our observed data well. In the case of the best fit model above, m is close to 1, and b is just a bit larger than 0.
 

And why do we care about this? Well, that value of m can be really informative. If m is very large and positive, then a small change in the value of x predicts a tremendous positive change in the value of y. If m is small, then changes in x predict small changes in y. A large m implies that x may have a large effect on y, hence m is also sometimes called the effect size. It’s also sometimes called a coefficient.
 
The principals of this model-fitting can be applied to linear models created on data with many more parameters – many dimensions. But they are fit in similar ways. We may not be able to draw multidimensional datasets in neat graphs but we can still apply least-squares to them!
 

Testing for significance

Now that we’ve fit a model and found values for m and b, we’d like to know something: does m really matter?

For an independent data set, that is, the best-fit linear model could have the exact same values for m and b. But does the value of x make an accurate prediction of the value of y?

That’s why many model-fitting tools return not only a slope for each parameter, but a p-value. This p-value is an indicator of whether that predictor (x) is actually useful in informing you about the state of the response variable (y).

To assess whether a parameter is predictive, we remove the variable (x) and it’s coefficient (m) from the model. And then we see how good we are at predicting y with a model that doesn’t include them.
In this case a model with no x means that we guess: we create a model where the prediction for y is always the same: the mean value of all of the observed values of y.

We compare the predictions between these two models. If our model that includes x is much better at prediction, we assign a low p-value to that coefficient.

We do this kind of testing for significance in many statistical settings, including one of my favorites: testing for differential expression of genes in RNA-Seq experiments. If a linear model that includes the expression level of gene A is better at predicting which group a sample comes from than a model without A, we decide that gene A is significantly differentially expressed.

>>> from sklearn.linear_model import LinearRegression
>>> ols = LinearRegression()
>>> ols.fit(features_train, target_train)
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
>>> ols.score(features_test, target_test)
0.6353620786674621

Logistic regression
Logistic regression model:- In statistics, the logistic model (or logit model) is a statistical model that is usually taken to apply to a binary dependent variable. In regression analysis, logistic regression or logit regression is estimating the parameters of a logistic model. More formally, a logistic model is one where the log-odds of the probability of an event is a linear combination of independent or predictor variables. The two possible dependent variable values are often labelled as "0" and "1", which represent outcomes such as pass/fail, win/lose, alive/dead or healthy/sick. The binary logistic regression model can be generalized to more than two levels of the dependent variable: categorical outputs with more than two values are modelled by multinomial logistic regression, and if the multiple categories are ordered, by ordinal logistic regression, for example the proportional odds ordinal logistic model.[1]
Logistic regression was developed by statistician David Cox in 1958.[1][2] The binary logistic model is used to estimate the probability of a binary response based on one or more predictor (or independent) variables (features). It allows one to say that the presence of a risk factor increases the odds of a given outcome by a specific factor. The model itself simply models probability of output in terms of input, and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other. The coefficients are generally not computed by a closed-form expression, unlike linear least squares.

k-nearest neighbors algorithm
In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression.[1] In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:

    In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (kis a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.

    In k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.

k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.
Both for classification and regression, a useful technique can be used to assign weight to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.[2]
The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.

NumPy (Numeric Python)



SciPy(Scientific Python)



training data
test data
independent data set

input (x-vectors/labels/features) and target data ------> algorithm ----------> predictive model

independent data set -----> predictive model ------> prediction

compare prediction to actual and then get the accuracy of the model

data munging

k-folds (breaks input data in to subsets, uses subsets for training and test, gets average of operational runs)
holdout (breaks input data in to 75/25 ratio, training to test)

supervised, semi-supervised and unsupervised learning

regression
classification
clustering

supervised (training input has corresponding targets)
semi-supervised (some targets are missing)
unsupervised (data does not have targets, data mining, searching for patterns)

machine learning algorithms: learn a target function that best maps input variables (X)
    to an output variable (Y), Y = f(X)

X ----> target function ----> Y

input example

[1,2,3,4,5,6,7,8,9,10]

output/target example

[1,4,9,16,25,36,49,64,81,100]

target function

f(X) == x^2

X ----> x^2 ----> Y

independent data set

[42,47,59,19,23]

independent data set output

[1764, 2209, 3481, 361, 529]

supervised learning/predictive modeling
    search for the best function that maps input data to output results

Linear Regression

Logistic Regression

Linear Discriminant Analysis (LDA)

Classification and Regression Trees

Naive Bayes

K-Nearest Neighbors
    k is the number of neighbors

Learning Vector Quantization (LVQ)

Support Vector Machines

Bagging and Random Forest

Boosting and AdaBoost

Support Vector Machines.............................................
SVM

Hyperplane
In geometry, a hyperplane is a subspace whose dimension is one less than that of its ambient space. If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes, while if the space is 2-dimensional, its hyperplanes are the 1-dimensional lines. This notion can be used in any general space in which the concept of the dimension of a subspace is defined.
In different settings, the objects which are hyperplanes may have different properties. For instance, a hyperplane of an n-dimensional affine space is a flat subset with dimension n − 1. By its nature, it separates the space into two half spaces. A hyperplane of an n-dimensional projective space does not have this property.

support vector classifier (SVC)
    linear classifier that maximizes the margins between the classes
    SVC works well in high dimensions
    SVC attempts to find the hyperplane 

hyperplane is boundary deciding how new observations are classified


Kernel Method
In machine learning, kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM). The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets.


Vector Space
A vector space (also called a linear space) is a collection of objects called vectors, which may be added together and multiplied ("scaled") by numbers, called scalars. Scalars are often taken to be real numbers, but there are also vector spaces with scalar multiplication by complex numbers, rational numbers, or generally any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called axioms, listed below.

Euclidean vectors are an example of a vector space. They represent physical quantities such as forces: any two forces (of the same type) can be added to yield a third, and the multiplication of a force vector by a real multiplier is another force vector. In the same vein, but in a more geometric sense, vectors representing displacements in the plane or in three-dimensional space also form vector spaces. Vectors in vector spaces do not necessarily have to be arrow-like objects as they appear in the mentioned examples: vectors are regarded as abstract mathematical objects with particular properties, which in some cases can be visualized as arrows.


kernel linear/polynomial

regression analysis is a set of statistical processes for estimating the relationship among variables


support vectors make up the hyperplane which helps to break up data into classifications

clustering
    find patterns in the data, useful when missing target vectors

clustering
    K-Means
        group observations into k groups
    Meanshift
    DBSCAN
    Hierarchical Merging

k-means clustering
    k cluster center points are created at random locations
    for observation
        distance between observation and the k center points is calculated
        observation is assigned to the cluster of the nearest center point
    center points are moved to the means of their respective clusters

k-means groups are balanced (have roughly the same number of observations)

kmeans, k is number of clusters; k nearest neighbors, k is number of neighbors in neighborhood

neural networks
    observation feature values ----> network layers ----> target value
    input layer
    value weights
    middle layers
    target layer

About this course: Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems.

Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI. This course provides a broad introduction to machine learning, datamining, and statistical pattern recognition. Topics include: (i) Supervised learning (parametric/non-parametric algorithms, support vector machines, kernels, neural networks). (ii) Unsupervised learning (clustering, dimensionality reduction, recommender systems, deep learning). (iii) Best practices in machine learning (bias/variance theory; innovation process in machine learning and AI). The course will also draw from numerous case studies and applications, so that you'll also learn how to apply learning algorithms to building smart robots (perception, control), text understanding (web search, anti-spam), computer vision, medical informatics, audio, database mining, and other areas.

Coursera Machine Learning Syllabus

Intro

Video: What is Machine Learning?
Reading: What is Machine Learning?
Video: Supervised Learning
Reading: Supervised Learning
Video: Unsupervised Learning
Reading: Unsupervised Learning


Linear Regression with One Variable

Linear regression predicts a real-valued output based on an input value. We discuss the application of linear regression to housing price prediction, present the notion of a cost function, and introduce the gradient descent method for learning.

    Video: Model Representation
    Reading: Model Representation
    Video: Cost Function
    Reading: Cost Function
    Video: Cost Function - Intuition I
    Reading: Cost Function - Intuition I
    Video: Cost Function - Intuition II
    Reading: Cost Function - Intuition II
    Video: Gradient Descent
    Reading: Gradient Descent
    Video: Gradient Descent Intuition
    Reading: Gradient Descent Intuition
    Video: Gradient Descent For Linear Regression
    Reading: Gradient Descent For Linear Regression
    Reading: Lecture Slides


Linear Algebra Review
This optional module provides a refresher on linear algebra concepts. Basic understanding of linear algebra is necessary for the rest of the course, especially as we begin to cover models with multiple variables.

    Video: Matrices and Vectors
    Reading: Matrices and Vectors
    Video: Addition and Scalar Multiplication
    Reading: Addition and Scalar Multiplication
    Video: Matrix Vector Multiplication
    Reading: Matrix Vector Multiplication
    Video: Matrix Matrix Multiplication
    Reading: Matrix Matrix Multiplication
    Video: Matrix Multiplication Properties
    Reading: Matrix Multiplication Properties
    Video: Inverse and Transpose
    Reading: Inverse and Transpose
    Reading: Lecture Slides
    Practice Quiz: Linear Algebra

Linear Regression with Multiple Variables
What if your input has more than one value? In this module, we show how linear regression can be extended to accommodate multiple input features. We also discuss best practices for implementing linear regression.
8 videos, 16 readings

    Reading: Setting Up Your Programming Assignment Environment
    Reading: Accessing MATLAB Online and Uploading the Exercise Files
    Reading: Installing Octave on Windows
    Reading: Installing Octave on Mac OS X (10.10 Yosemite and 10.9 Mavericks and Later)
    Reading: Installing Octave on Mac OS X (10.8 Mountain Lion and Earlier)
    Reading: Installing Octave on GNU/Linux
    Reading: More Octave/MATLAB resources
    Video: Multiple Features
    Reading: Multiple Features
    Video: Gradient Descent for Multiple Variables
    Reading: Gradient Descent For Multiple Variables
    Video: Gradient Descent in Practice I - Feature Scaling
    Reading: Gradient Descent in Practice I - Feature Scaling
    Video: Gradient Descent in Practice II - Learning Rate
    Reading: Gradient Descent in Practice II - Learning Rate
    Video: Features and Polynomial Regression
    Reading: Features and Polynomial Regression
    Video: Normal Equation
    Reading: Normal Equation
    Video: Normal Equation Noninvertibility
    Reading: Normal Equation Noninvertibility
    Video: Working on and Submitting Programming Assignments
    Reading: Programming tips from Mentors
    Reading: Lecture Slides

Octave/Matlab Tutorial
This course includes programming assignments designed to help you understand how to implement the learning algorithms in practice. To complete the programming assignments, you will need to use Octave or MATLAB. This module introduces Octave/Matlab and shows you how to submit an assignment.
6 videos, 1 reading

    Video: Basic Operations
    Video: Moving Data Around
    Video: Computing on Data
    Video: Plotting Data
    Video: Control Statements: for, while, if statement
    Video: Vectorization
    Reading: Lecture Slides
    Programming: Linear Regression

Logistic Regression
Logistic regression is a method for classifying data into discrete outcomes. For example, we might use logistic regression to classify an email as spam or not spam. In this module, we introduce the notion of classification, the cost function for logistic regression, and the application of logistic regression to multi-class classification.
7 videos, 8 readings

    Video: Classification
    Reading: Classification
    Video: Hypothesis Representation
    Reading: Hypothesis Representation
    Video: Decision Boundary
    Reading: Decision Boundary
    Video: Cost Function
    Reading: Cost Function
    Video: Simplified Cost Function and Gradient Descent
    Reading: Simplified Cost Function and Gradient Descent
    Video: Advanced Optimization
    Reading: Advanced Optimization
    Video: Multiclass Classification: One-vs-all
    Reading: Multiclass Classification: One-vs-all
    Reading: Lecture Slides

Regularization
Machine learning models need to generalize well to new examples that the model has not seen in practice. In this module, we introduce regularization, which helps prevent models from overfitting the training data.
4 videos, 5 readings

    Video: The Problem of Overfitting
    Reading: The Problem of Overfitting
    Video: Cost Function
    Reading: Cost Function
    Video: Regularized Linear Regression
    Reading: Regularized Linear Regression
    Video: Regularized Logistic Regression
    Reading: Regularized Logistic Regression
    Reading: Lecture Slides
    Programming: Logistic Regression

Neural Networks: Representation
Neural networks is a model inspired by how the brain works. It is widely used today in many applications: when your phone interprets and understand your voice commands, it is likely that a neural network is helping to understand your speech; when you cash a check, the machines that automatically read the digits also use neural networks.
7 videos, 6 readings

    Video: Non-linear Hypotheses
    Video: Neurons and the Brain
    Video: Model Representation I
    Reading: Model Representation I
    Video: Model Representation II
    Reading: Model Representation II
    Video: Examples and Intuitions I
    Reading: Examples and Intuitions I
    Video: Examples and Intuitions II
    Reading: Examples and Intuitions II
    Video: Multiclass Classification
    Reading: Multiclass Classification
    Reading: Lecture Slides
    Programming: Multi-class Classification and Neural Networks

Neural Networks: Learning
In this module, we introduce the backpropagation algorithm that is used to help learn parameters for a neural network. At the end of this module, you will be implementing your own neural network for digit recognition.
8 videos, 8 readings

    Video: Cost Function
    Reading: Cost Function
    Video: Backpropagation Algorithm
    Reading: Backpropagation Algorithm
    Video: Backpropagation Intuition
    Reading: Backpropagation Intuition
    Video: Implementation Note: Unrolling Parameters
    Reading: Implementation Note: Unrolling Parameters
    Video: Gradient Checking
    Reading: Gradient Checking
    Video: Random Initialization
    Reading: Random Initialization
    Video: Putting It Together
    Reading: Putting It Together
    Video: Autonomous Driving
    Reading: Lecture Slides
    Programming: Neural Network Learning

Advice for Applying Machine Learning
Applying machine learning in practice is not always straightforward. In this module, we share best practices for applying machine learning in practice, and discuss the best ways to evaluate performance of the learned models.
7 videos, 7 readings

    Video: Deciding What to Try Next
    Video: Evaluating a Hypothesis
    Reading: Evaluating a Hypothesis
    Video: Model Selection and Train/Validation/Test Sets
    Reading: Model Selection and Train/Validation/Test Sets
    Video: Diagnosing Bias vs. Variance
    Reading: Diagnosing Bias vs. Variance
    Video: Regularization and Bias/Variance
    Reading: Regularization and Bias/Variance
    Video: Learning Curves
    Reading: Learning Curves
    Video: Deciding What to Do Next Revisited
    Reading: Deciding What to do Next Revisited
    Reading: Lecture Slides
    Programming: Regularized Linear Regression and Bias/Variance

Machine Learning System Design
To optimize a machine learning algorithm, you’ll need to first understand where the biggest improvements can be made. In this module, we discuss how to understand the performance of a machine learning system with multiple parts, and also how to deal with skewed data.
5 videos, 3 readings

    Video: Prioritizing What to Work On
    Reading: Prioritizing What to Work On
    Video: Error Analysis
    Reading: Error Analysis
    Video: Error Metrics for Skewed Classes
    Video: Trading Off Precision and Recall
    Video: Data For Machine Learning
    Reading: Lecture Slides

Support Vector Machines
Support vector machines, or SVMs, is a machine learning algorithm for classification. We introduce the idea and intuitions behind SVMs and discuss how to use it in practice.
6 videos, 1 reading

    Video: Optimization Objective
    Video: Large Margin Intuition
    Video: Mathematics Behind Large Margin Classification
    Video: Kernels I
    Video: Kernels II
    Video: Using An SVM
    Reading: Lecture Slides
    Programming: Support Vector Machines

Unsupervised Learning
We use unsupervised learning to build models that help us understand our data better. We discuss the k-Means algorithm for clustering that enable us to learn groupings of unlabeled data points.
5 videos, 1 reading

    Video: Unsupervised Learning: Introduction
    Video: K-Means Algorithm
    Video: Optimization Objective
    Video: Random Initialization
    Video: Choosing the Number of Clusters
    Reading: Lecture Slides

Dimensionality Reduction
In this module, we introduce Principal Components Analysis, and show how it can be used for data compression to speed up learning algorithms as well as for visualizations of complex datasets.
7 videos, 1 reading

    Video: Motivation I: Data Compression
    Video: Motivation II: Visualization
    Video: Principal Component Analysis Problem Formulation
    Video: Principal Component Analysis Algorithm
    Video: Reconstruction from Compressed Representation
    Video: Choosing the Number of Principal Components
    Video: Advice for Applying PCA
    Reading: Lecture Slides
    Programming: K-Means Clustering and PCA

Anomaly Detection
Given a large number of data points, we may sometimes want to figure out which ones vary significantly from the average. For example, in manufacturing, we may want to detect defects or anomalies. We show how a dataset can be modeled using a Gaussian distribution, and how the model can be used for anomaly detection.
8 videos, 1 reading

    Video: Problem Motivation
    Video: Gaussian Distribution
    Video: Algorithm
    Video: Developing and Evaluating an Anomaly Detection System
    Video: Anomaly Detection vs. Supervised Learning
    Video: Choosing What Features to Use
    Video: Multivariate Gaussian Distribution
    Video: Anomaly Detection using the Multivariate Gaussian Distribution
    Reading: Lecture Slides

Recommender Systems
When you buy a product online, most websites automatically recommend other products that you may like. Recommender systems look at patterns of activities between different users and different products to produce these recommendations. In this module, we introduce recommender algorithms such as the collaborative filtering algorithm and low-rank matrix factorization.
Show less
6 videos, 1 reading

    Video: Problem Formulation
    Video: Content Based Recommendations
    Video: Collaborative Filtering
    Video: Collaborative Filtering Algorithm
    Video: Vectorization: Low Rank Matrix Factorization
    Video: Implementational Detail: Mean Normalization
    Reading: Lecture Slides
    Programming: Anomaly Detection and Recommender Systems

Large Scale Machine Learning
Machine learning works best when there is an abundance of data to leverage for training. In this module, we discuss how to apply the machine learning algorithms with large datasets.
6 videos, 1 reading

    Video: Learning With Large Datasets
    Video: Stochastic Gradient Descent
    Video: Mini-Batch Gradient Descent
    Video: Stochastic Gradient Descent Convergence
    Video: Online Learning
    Video: Map Reduce and Data Parallelism
    Reading: Lecture Slides

Application Example: Photo OCR
Identifying and recognizing objects, words, and digits in an image is a challenging task. We discuss how a pipeline can be built to tackle this problem and how to analyze and improve the performance of such a system.
5 videos, 1 reading

    Video: Problem Description and Pipeline
    Video: Sliding Windows
    Video: Getting Lots of Data and Artificial Data
    Video: Ceiling Analysis: What Part of the Pipeline to Work on Next
    Reading: Lecture Slides
    Video: Summary and Thank You


overfitting training set
nth power of polynomial
d = degree of polynomial
linear equation, square equation, cubic equation
5th order polynomial
generalize hypothesis
training set
cv set
test set
data to fit parameters might work better on model
    than other data sets
model selection
data subset (75/25, 70/30, 60/20/20)
cross validation
60% training set
20% cross validation set
20% test set
run cross validation set through nth power model equations
cross validation data selects the model
diagnosing bias vs. variance
high bias underfit data
high variance overfit data
goodness of fit
learning curves
clustering
    k-means (k clusters)
    agglomerative clustering, hierarchy of clusters
k-folds
k-means, k number of clusters
linear regression
    y = mx + b
    coefficients weights
linear algebra
    matrices and vectors
linear regression
logistic regression
    sigmoid function
regularization
    choose the regularization value lambda
        to get an appropriate fit
neural networks
machine learning system design
support vector machines
    hyperplane
unsupervised learning
    clustering
    k-means
dimensionality reduction
anomaly detection
recommender systems
large scale machine learning

towardsdatascience.com
    design patterns
        layers
        mvc, model-view-controller

standard deviation
    find vector mean value
    for each vector value subtract the mean and square the result
    find the mean value of the squared differences
    take square root of the squared diff mean value

Scikit
TensorFlow
Hadoop/Spark
Python Module
DeepLearning.ai
Bagging
    Ensemble
Random Forests
GBM Gradient Boosting Machines
Support Vector Machine
    Kernels
Neural Networks
80% of the work is data munging
Stochastic
Venture Capital (VCs)

DraftKings scoring system

Passing TD    +4 Pts
25 Passing Yards    +1 Pt (0.04 Pts/Yards)
300+ Yard Passing Game    +3 Pts
Interception    -1 Pt
Rushing TD    +6 Pts
10 Rushing Yards    +1 Pt (0.1 Pts/Yards)
100+ Yard Rushing Game    +3 Pts
Receiving TD    +6 Pts
10 Receiving Yards    +1 Pt (0.1 Pts/Yards)
100+ Receiving Yard Game    +3 Pts
Reception    +1 Pt

>>> def qbPts(pass_yds, pass_td, pass_int, rush_td, rush_yds):
...     pts = 0
...     pts = pts + (pass_td * 4)
...     if pass_yds >= 300:
...         pts = pts + 3
...     pts = pts + floor(pass_yds / 25)
...     pts = pts - pass_int
...     pts = pts + (rush_td * 6)
...     pts = pts + floor(rush_yds / 10)
...     return pts
...
>>> patrick_mahomes_pts = qbPts(314, 3, 0, 0, 7)
>>> patrick_mahomes_pts
27

>>> def rbPts(rush_yds, rush_td, recv_yds, recv_td, receptions):
...     pts = 0
...     pts = pts + floor(rush_yds / 10)
...     if rush_yds >= 100:
...         pts = pts + 3
...     pts = pts + (rush_td * 6)
...     pts = pts + floor(recv_yds / 10)
...     if recv_yds >= 100:
...         pts = pts + 3
...     pts = pts + (recv_td * 6)
...     pts = pts + receptions
...     return pts
...
>>> todd_gurley_pts = rbPts(105, 1, 51, 0, 5)
>>> todd_gurley_pts
29

TensorFlow

tensor is a vector/matrix

Scikit learn
Python is slower, C++ is faster

MNIST dataset

60,000 training set examples
10,000 test set examples
images with 28x28 pixels

input layer > weights > layer 2 activation function > weights >
    layer 3 activation function > weights > output layer
    
compare output to intended output > cost function (cross entropy)
    
optimization function (optimizer) > minimize cost

feedforward
backpropagation

setup the neural network model

input layer has 784 pixels per image (28x28)
output layer has 10 classes

minimize cost improve model

cross entropy
stochastic gradient descent

train data, reduce cost, improve accuracy

stochastic gradient descent (sgd) optimizes a differentiable objective function
    by randomly selecting samples (shuffling)
    
stochastic gradient descent is a method for improving model accuracy

//////////////////////////////////////////////////////////////////////////////
// python code
//////////////////////////////////////////////////////////////////////////////

from sklearn.linear_model import LinearRegression
import numpy as np

>>> batting_dtype = [
... ('Player',np.unicode_),
... ('Team',np.unicode_),
... ('G',int),
... ('PA',int),
... ('AB',int),
... ('HR',int),
... ('AVG',float),
... ('OBP',float),
... ('SLG',float),
... ('OPS',float),
... ('OPS_PLUS',int),
... ('WAR',float)
... ]
>>> redsox_batting = np.genfromtxt('redsox-batting-092118.txt',dtype=batting_dtype,skip_header=1,delimiter=',')

import pandas as pd
>>> redsox_bat_df = pd.read_csv('redsox-batting-092118.txt')
>>> redsox_bat_df = redsox_bat_df.set_index(redsox_bat_df['Player'])
                  Player Team    G   PA   AB  HR    AVG    OBP    SLG    OPS  OPS_PLUS  WAR
Player
Betts              Betts  BOS  131  593  502  30  0.339  0.432  0.625  1.057       182         10.2
Martinez        Martinez  BOS  143  622  547  39  0.331  0.400  0.620  1.029       172          6.0
Kinsler          Kinsler  BOS   31  114  107   1  0.271  0.307  0.336  0.643        75          0.0
Pearce            Pearce  BOS   44  140  114   6  0.298  0.414  0.535  0.949       151          1.3
Benintendi    Benintendi  BOS  142  630  552  16  0.286  0.362  0.466  0.828       122          3.9
Bogaerts        Bogaerts  BOS  130  555  493  21  0.284  0.353  0.511  0.864       130          3.4
Nunez              Nunez  BOS  125  494  473  10  0.264  0.288  0.389  0.677        81         -1.2
Holt                Holt  BOS  104  345  302   6  0.258  0.345  0.358  0.703       104          1.2
Moreland        Moreland  BOS  119  436  387  15  0.243  0.317  0.434  0.751       101          0.8
Devers            Devers  BOS  104  458  422  18  0.242  0.297  0.422  0.719        92         -0.1
Bradley Jr.  Bradley Jr.  BOS  126  507  449  13  0.232  0.312  0.403  0.715        92          2.1
Swihart          Swihart  BOS   75  184  170   1  0.229  0.288  0.300  0.588        60         -0.2

>>> X = redsox_bat_df[['SLG','OPS']]
>>> X = X.set_index(redsox_bat_df['Player'])
               SLG    OPS
Player
Betts        0.625  1.057
Martinez     0.620  1.029
Kinsler      0.336  0.643
Pearce       0.535  0.949
Benintendi   0.466  0.828
Bogaerts     0.511  0.864
Nunez        0.389  0.677
Holt         0.358  0.703
Moreland     0.434  0.751
Devers       0.422  0.719
Bradley Jr.  0.403  0.715
Swihart      0.300  0.588

>>> y = redsox_bat_df[['AVG']]
>>> y = y.set_index(redsox_bat_df['Player'])
               AVG
Player
Betts        0.339
Martinez     0.331
Kinsler      0.271
Pearce       0.298
Benintendi   0.286
Bogaerts     0.284
Nunez        0.264
Holt         0.258
Moreland     0.243
Devers       0.242
Bradley Jr.  0.232
Swihart      0.229

>>> model = regression.fit(X,y)
>>> test_set
array([[0.577, 0.968],
       [0.456, 0.855],
       [0.636, 1.096]])
>>> p = model.predict(test_set)
>>> p
array([[0.30812398],
       [0.29639237],
       [0.34356058]])
       
>>> test_set_df = pd.DataFrame(test_set, columns=['SLG','OPS'], index=['Yelich','Altuve','Trout'])
>>> test_set_df
          SLG    OPS
Yelich  0.577  0.968
Altuve  0.456  0.855
Trout   0.636  1.096
            
>>> test_set_df['AVG'] = [.322,.317,.316]
>>> test_set_df
          SLG    OPS    AVG
Yelich  0.577  0.968  0.322
Altuve  0.456  0.855  0.317
Trout   0.636  1.096  0.316    

>>> X = X.append(test_set_df[['SLG','OPS']])
>>> X
               SLG    OPS
Betts        0.625  1.057
Martinez     0.620  1.029
Kinsler      0.336  0.643
Pearce       0.535  0.949
Benintendi   0.466  0.828
Bogaerts     0.511  0.864
Nunez        0.389  0.677
Holt         0.358  0.703
Moreland     0.434  0.751
Devers       0.422  0.719
Bradley Jr.  0.403  0.715
Swihart      0.300  0.588
Yelich       0.577  0.968
Altuve       0.456  0.855
Trout        0.636  1.096
>>> y = y.append(test_set_df[['AVG']])
>>> y
               AVG
Betts        0.339
Martinez     0.331
Kinsler      0.271
Pearce       0.298
Benintendi   0.286
Bogaerts     0.284
Nunez        0.264
Holt         0.258
Moreland     0.243
Devers       0.242
Bradley Jr.  0.232
Swihart      0.229
Yelich       0.322
Altuve       0.317
Trout        0.316

>>> model = regression.fit(X,y)
>>> model.predict([[.450,.836]])[0]
array([0.29139216])

scale features attempt improved accuracy

>>> from sklearn.preprocessing import StandardScaler
>>> scaler = StandardScaler()
>>> X_standardized = scaler.fit_transform(X)
>>> model = regression.fit(X_standardized,y)
>>> model.predict([[.450,.836]])[0]
array([0.32090764])

improved, batting average of player is .314

lots of noise, model is not that good

>>> test_set_df['OBP'] = [.391,.388,.460]
>>> test_set_df
          SLG    OPS    AVG    OBP
Yelich  0.577  0.968  0.322  0.391
Altuve  0.456  0.855  0.317  0.388
Trout   0.636  1.096  0.316  0.460

>>> X = X.append(test_set_df[['OBP','OPS']])
>>> X
               OBP    OPS
Betts        0.432  1.057
Martinez     0.400  1.029
Kinsler      0.307  0.643
Pearce       0.414  0.949
Benintendi   0.362  0.828
Bogaerts     0.353  0.864
Nunez        0.288  0.677
Holt         0.345  0.703
Moreland     0.317  0.751
Devers       0.297  0.719
Bradley Jr.  0.312  0.715
Swihart      0.288  0.588
Yelich       0.391  0.968
Altuve       0.388  0.855
Trout        0.460  1.096

>>> regression = LinearRegression()
>>> model = regression.fit(X,y)

>>> model.predict([[.354,.874]])[0]
array([0.28713072])

player average is .297

>>> scaler = StandardScaler()
>>> X_std = scaler.fit_transform(X)
>>> model = regression.fit(X_std,y)
>>> model.predict([[.339,.753]])[0]
array([0.30133933])

player average is .303

>>> X = redsox_bat_df[['OBP','SLG','OPS']]
>>> X
               OBP    SLG    OPS
Player
Betts        0.432  0.625  1.057
Martinez     0.400  0.620  1.029
Kinsler      0.307  0.336  0.643
Pearce       0.414  0.535  0.949
Benintendi   0.362  0.466  0.828
Bogaerts     0.353  0.511  0.864
Nunez        0.288  0.389  0.677
Holt         0.345  0.358  0.703
Moreland     0.317  0.434  0.751
Devers       0.297  0.422  0.719
Bradley Jr.  0.312  0.403  0.715
Swihart      0.288  0.300  0.588

>>> X = X.append(test_set_df[['OBP','SLG','OPS']])
>>> X
               OBP    SLG    OPS
Betts        0.432  0.625  1.057
Martinez     0.400  0.620  1.029
Kinsler      0.307  0.336  0.643
Pearce       0.414  0.535  0.949
Benintendi   0.362  0.466  0.828
Bogaerts     0.353  0.511  0.864
Nunez        0.288  0.389  0.677
Holt         0.345  0.358  0.703
Moreland     0.317  0.434  0.751
Devers       0.297  0.422  0.719
Bradley Jr.  0.312  0.403  0.715
Swihart      0.288  0.300  0.588
Yelich       0.391  0.577  0.968
Altuve       0.388  0.456  0.855
Trout        0.460  0.636  1.096

>>> import pandas as pd
>>> import numpy as np
>>> batting_leaders_df = pd.read_table('batting-leaders-092518.txt')
>>> batting_leaders_df
         Player Team Pos    G   AB    R    H  2B  3B  HR  RBI   BB   SO  SB  CS    AVG    OBP    SLG    OPS
0         Betts  BOS  RF  133  513  125  176  46   5  32   80   78   90  29   6  0.343  0.434  0.639  1.073
1      Martinez  BOS  LF  146  555  106  182  36   2  41  124   69  143   6   1  0.328  0.402  0.622  1.023
2        Yelich  MIL  LF  141  557  109  179  34   6  32   98   58  130  21   4  0.321  0.391  0.576  0.967
3        Altuve  HOU  2B  133  520   84  165  29   2  13   61   55   77  17   4  0.317  0.388  0.456  0.844
4       Gennett  CIN  2B  151  571   85  180  30   2  23   92   42  123   4   2  0.315  0.363  0.496  0.859
5         Trout  LAA  CF  135  459   98  144  24   4  38   77  117  121  24   2  0.314  0.459  0.632  1.091
6          Cain  MIL  CF  135  513   85  160  24   2  10   37   69   88  29   7  0.312  0.400  0.425  0.825
7       Zobrist  CHC  2B  133  431   64  134  27   2   9   57   53   58   3   4  0.311  0.383  0.445  0.829
8       Freeman  ATL  1B  156  599   94  186  40   4  23   95   74  125  10   3  0.311  0.389  0.506  0.895
9      Brantley  CLE  LF  138  554   86  171  35   2  17   75   46   55  11   3  0.309  0.363  0.471  0.834
10       Rendon  WSH  3B  131  510   85  157  42   1  23   87   54   80   2   1  0.308  0.373  0.529  0.903
11     Martinez  STL  1B  147  515   62  157  28   0  17   83   46  102   0   3  0.305  0.362  0.458  0.820
12   Merrifield   KC  2B  152  606   86  184  42   3  12   58   59  109  38  10  0.304  0.368  0.442  0.810
13       Segura  SEA  SS  139  568   87  172  29   3   9   61   30   68  20  11  0.303  0.339  0.412  0.751
14  Castellanos  DET  RF  151  597   83  180  45   5  22   86   47  145   2   1  0.302  0.357  0.504  0.862
15     Markakis  ATL  RF  156  604   77  182  43   2  14   93   68   74   1   1  0.301  0.368  0.449  0.817
16       Wendle   TB  2B  134  469   58  141  31   6   7   57   36   94  15   4  0.301  0.356  0.437  0.793
17      Andujar  NYY  3B  143  547   78  163  43   2  25   85   24   94   2   1  0.298  0.330  0.521  0.851
18        Smith   TB  CF  135  455   60  135  26   9   2   38   46   93  36  12  0.297  0.369  0.407  0.775
19      Simmons  LAA  SS  142  540   66  160  26   5  11   74   33   42   9   2  0.296  0.340  0.424  0.764
20      Machado  LAD  SS  156  608   82  180  34   2  37  104   69  103  14   2  0.296  0.367  0.541  0.908
21      Peralta  ARI  LF  142  544   74  161  24   5  29   84   45  118   4   0  0.296  0.353  0.518  0.871
22    Dickerson  PIT  LF  131  487   63  144  33   7  12   53   20   77   8   3  0.296  0.326  0.466  0.792
23        Duffy   TB  3B  131  503   58  148  22   1   4   44   46   93  12   6  0.294  0.360  0.366  0.726
24      Arenado  COL  3B  149  561   99  165  35   2  34  104   72  119   2   2  0.294  0.374  0.545  0.919
25  Goldschmidt  ARI  1B  154  576   94  169  35   5  33   83   89  167   6   4  0.293  0.393  0.543  0.937
26         Baez  CHC  2B  153  578   97  169  38   9  34  110   27  158  21   9  0.292  0.327  0.566  0.893
27       Peraza  CIN  SS  152  611   84  177  31   4  13   56   29   72  23   6  0.290  0.328  0.417  0.746
28      Gurriel  HOU  1B  132  522   69  151  33   1  13   85   23   61   5   1  0.289  0.323  0.431  0.754
29        Story  COL  SS  150  574   82  166  42   5  33  102   43  165  26   6  0.289  0.344  0.552  0.896
30      Rosario  MIN  LF  138  559   87  161  31   2  24   77   30  104   8   2  0.288  0.323  0.479  0.803
31      Bregman  HOU  3B  153  580  103  167  51   1  30  101   93   80  10   4  0.288  0.395  0.534  0.930
32     Blackmon  COL  CF  149  599  112  172  28   6  27   65   57  132  12   4  0.287  0.356  0.489  0.845
33     Bogaerts  BOS  SS  133  503   68  144  43   3  21   96   53  100   8   2  0.286  0.357  0.509  0.866
34   Benintendi  BOS  LF  145  565  100  161  39   6  16   84   71  104  21   3  0.285  0.363  0.460  0.823
35      Haniger  SEA  RF  152  576   87  164  37   4  26   91   68  142   8   2  0.285  0.367  0.498  0.865
36       Suarez  CIN  3B  138  510   76  145  22   2  32  101   63  137   1   1  0.284  0.369  0.524  0.893
37         Kemp  LAD  LF  142  454   60  129  23   0  21   82   36  113   0   0  0.284  0.333  0.474  0.807
38        Votto  CIN  1B  140  486   66  138  27   2  12   67  105   98   1   0  0.284  0.418  0.422  0.840
39       Lindor  CLE  SS  152  634  124  178  42   2  36   89   67  105  23   9  0.281  0.355  0.524  0.879
40     Realmuto  MIA   C  122  467   74  131  30   3  21   73   38  100   3   2  0.281  0.345  0.493  0.837
41        Jones  BAL  CF  139  558   53  156  32   0  15   61   23   89   7   1  0.280  0.310  0.418  0.727
42      Chapman  OAK  3B  141  533   98  149  42   6  24   68   56  142   1   2  0.280  0.357  0.516  0.873
43     LeMahieu  COL  2B  121  505   86  141  32   2  15   59   36   79   6   5  0.279  0.326  0.440  0.766
44       Castro  MIA  2B  151  582   76  162  31   2  12   54   47  122   6   3  0.278  0.329  0.400  0.729
45        Rizzo  CHC  1B  146  540   69  150  26   1  24   96   68   80   6   4  0.278  0.375  0.463  0.838
46        Ozuna  STL  LF  144  568   68  157  16   2  23   87   36  107   3   0  0.276  0.321  0.433  0.754
47        Mauer  MIN  1B  120  457   55  126  25   1   6   46   47   83   0   0  0.276  0.344  0.374  0.718
48        Marte  PIT  CF  139  537   76  148  27   5  19   69   29  107  33  14  0.276  0.321  0.451  0.771
49      Ramirez  CLE  3B  151  555  104  152  37   4  38  103  103   76  33   6  0.274  0.392  0.560  0.952
>>> batting_leaders_df = batting_leaders_df.set_index(batting_leaders_df['Player']
... )
>>> batting_leaders_df
                  Player Team Pos    G   AB    R    H  2B  3B  ...    RBI   BB   SO  SB  CS    AVG    OBP    SLG    OPS
Player                                                         ...
Betts              Betts  BOS  RF  133  513  125  176  46   5  ...     80   78   90  29   6  0.343  0.434  0.639  1.073
Martinez        Martinez  BOS  LF  146  555  106  182  36   2  ...    124   69  143   6   1  0.328  0.402  0.622  1.023
Yelich            Yelich  MIL  LF  141  557  109  179  34   6  ...     98   58  130  21   4  0.321  0.391  0.576  0.967
Altuve            Altuve  HOU  2B  133  520   84  165  29   2  ...     61   55   77  17   4  0.317  0.388  0.456  0.844
Gennett          Gennett  CIN  2B  151  571   85  180  30   2  ...     92   42  123   4   2  0.315  0.363  0.496  0.859
Trout              Trout  LAA  CF  135  459   98  144  24   4  ...     77  117  121  24   2  0.314  0.459  0.632  1.091
Cain                Cain  MIL  CF  135  513   85  160  24   2  ...     37   69   88  29   7  0.312  0.400  0.425  0.825
Zobrist          Zobrist  CHC  2B  133  431   64  134  27   2  ...     57   53   58   3   4  0.311  0.383  0.445  0.829
Freeman          Freeman  ATL  1B  156  599   94  186  40   4  ...     95   74  125  10   3  0.311  0.389  0.506  0.895
Brantley        Brantley  CLE  LF  138  554   86  171  35   2  ...     75   46   55  11   3  0.309  0.363  0.471  0.834
Rendon            Rendon  WSH  3B  131  510   85  157  42   1  ...     87   54   80   2   1  0.308  0.373  0.529  0.903
Martinez        Martinez  STL  1B  147  515   62  157  28   0  ...     83   46  102   0   3  0.305  0.362  0.458  0.820
Merrifield    Merrifield   KC  2B  152  606   86  184  42   3  ...     58   59  109  38  10  0.304  0.368  0.442  0.810
Segura            Segura  SEA  SS  139  568   87  172  29   3  ...     61   30   68  20  11  0.303  0.339  0.412  0.751
Castellanos  Castellanos  DET  RF  151  597   83  180  45   5  ...     86   47  145   2   1  0.302  0.357  0.504  0.862
Markakis        Markakis  ATL  RF  156  604   77  182  43   2  ...     93   68   74   1   1  0.301  0.368  0.449  0.817
Wendle            Wendle   TB  2B  134  469   58  141  31   6  ...     57   36   94  15   4  0.301  0.356  0.437  0.793
Andujar          Andujar  NYY  3B  143  547   78  163  43   2  ...     85   24   94   2   1  0.298  0.330  0.521  0.851
Smith              Smith   TB  CF  135  455   60  135  26   9  ...     38   46   93  36  12  0.297  0.369  0.407  0.775
Simmons          Simmons  LAA  SS  142  540   66  160  26   5  ...     74   33   42   9   2  0.296  0.340  0.424  0.764
Machado          Machado  LAD  SS  156  608   82  180  34   2  ...    104   69  103  14   2  0.296  0.367  0.541  0.908
Peralta          Peralta  ARI  LF  142  544   74  161  24   5  ...     84   45  118   4   0  0.296  0.353  0.518  0.871
Dickerson      Dickerson  PIT  LF  131  487   63  144  33   7  ...     53   20   77   8   3  0.296  0.326  0.466  0.792
Duffy              Duffy   TB  3B  131  503   58  148  22   1  ...     44   46   93  12   6  0.294  0.360  0.366  0.726
Arenado          Arenado  COL  3B  149  561   99  165  35   2  ...    104   72  119   2   2  0.294  0.374  0.545  0.919
Goldschmidt  Goldschmidt  ARI  1B  154  576   94  169  35   5  ...     83   89  167   6   4  0.293  0.393  0.543  0.937
Baez                Baez  CHC  2B  153  578   97  169  38   9  ...    110   27  158  21   9  0.292  0.327  0.566  0.893
Peraza            Peraza  CIN  SS  152  611   84  177  31   4  ...     56   29   72  23   6  0.290  0.328  0.417  0.746
Gurriel          Gurriel  HOU  1B  132  522   69  151  33   1  ...     85   23   61   5   1  0.289  0.323  0.431  0.754
Story              Story  COL  SS  150  574   82  166  42   5  ...    102   43  165  26   6  0.289  0.344  0.552  0.896
Rosario          Rosario  MIN  LF  138  559   87  161  31   2  ...     77   30  104   8   2  0.288  0.323  0.479  0.803
Bregman          Bregman  HOU  3B  153  580  103  167  51   1  ...    101   93   80  10   4  0.288  0.395  0.534  0.930
Blackmon        Blackmon  COL  CF  149  599  112  172  28   6  ...     65   57  132  12   4  0.287  0.356  0.489  0.845
Bogaerts        Bogaerts  BOS  SS  133  503   68  144  43   3  ...     96   53  100   8   2  0.286  0.357  0.509  0.866
Benintendi    Benintendi  BOS  LF  145  565  100  161  39   6  ...     84   71  104  21   3  0.285  0.363  0.460  0.823
Haniger          Haniger  SEA  RF  152  576   87  164  37   4  ...     91   68  142   8   2  0.285  0.367  0.498  0.865
Suarez            Suarez  CIN  3B  138  510   76  145  22   2  ...    101   63  137   1   1  0.284  0.369  0.524  0.893
Kemp                Kemp  LAD  LF  142  454   60  129  23   0  ...     82   36  113   0   0  0.284  0.333  0.474  0.807
Votto              Votto  CIN  1B  140  486   66  138  27   2  ...     67  105   98   1   0  0.284  0.418  0.422  0.840
Lindor            Lindor  CLE  SS  152  634  124  178  42   2  ...     89   67  105  23   9  0.281  0.355  0.524  0.879
Realmuto        Realmuto  MIA   C  122  467   74  131  30   3  ...     73   38  100   3   2  0.281  0.345  0.493  0.837
Jones              Jones  BAL  CF  139  558   53  156  32   0  ...     61   23   89   7   1  0.280  0.310  0.418  0.727
Chapman          Chapman  OAK  3B  141  533   98  149  42   6  ...     68   56  142   1   2  0.280  0.357  0.516  0.873
LeMahieu        LeMahieu  COL  2B  121  505   86  141  32   2  ...     59   36   79   6   5  0.279  0.326  0.440  0.766
Castro            Castro  MIA  2B  151  582   76  162  31   2  ...     54   47  122   6   3  0.278  0.329  0.400  0.729
Rizzo              Rizzo  CHC  1B  146  540   69  150  26   1  ...     96   68   80   6   4  0.278  0.375  0.463  0.838
Ozuna              Ozuna  STL  LF  144  568   68  157  16   2  ...     87   36  107   3   0  0.276  0.321  0.433  0.754
Mauer              Mauer  MIN  1B  120  457   55  126  25   1  ...     46   47   83   0   0  0.276  0.344  0.374  0.718
Marte              Marte  PIT  CF  139  537   76  148  27   5  ...     69   29  107  33  14  0.276  0.321  0.451  0.771
Ramirez          Ramirez  CLE  3B  151  555  104  152  37   4  ...    103  103   76  33   6  0.274  0.392  0.560  0.952

[50 rows x 19 columns]

>>> X = batting_leaders_df[['OBP']]
>>> X
               OBP
Player
Betts        0.434
Martinez     0.402
Yelich       0.391
Altuve       0.388
Gennett      0.363
Trout        0.459
Cain         0.400
Zobrist      0.383
Freeman      0.389
Brantley     0.363
Rendon       0.373
Martinez     0.362
Merrifield   0.368
Segura       0.339
Castellanos  0.357
Markakis     0.368
Wendle       0.356
Andujar      0.330
Smith        0.369
Simmons      0.340
Machado      0.367
Peralta      0.353
Dickerson    0.326
Duffy        0.360
Arenado      0.374
Goldschmidt  0.393
Baez         0.327
Peraza       0.328
Gurriel      0.323
Story        0.344
Rosario      0.323
Bregman      0.395
Blackmon     0.356
Bogaerts     0.357
Benintendi   0.363
Haniger      0.367
Suarez       0.369
Kemp         0.333
Votto        0.418
Lindor       0.355
Realmuto     0.345
Jones        0.310
Chapman      0.357
LeMahieu     0.326
Castro       0.329
Rizzo        0.375
Ozuna        0.321
Mauer        0.344
Marte        0.321
Ramirez      0.392

>>> X['BB_AB'] = round(batting_leaders_df.loc[:,'BB'] / batting_leaders_df.loc[:,'AB'],3)
>>> X
               OBP  BB_AB
Player
Betts        0.434  0.152
Martinez     0.402  0.124
Yelich       0.391  0.104
Altuve       0.388  0.106
Gennett      0.363  0.074
Trout        0.459  0.255
Cain         0.400  0.135
Zobrist      0.383  0.123
Freeman      0.389  0.124
Brantley     0.363  0.083
Rendon       0.373  0.106
Martinez     0.362  0.089
Merrifield   0.368  0.097
Segura       0.339  0.053
Castellanos  0.357  0.079
Markakis     0.368  0.113
Wendle       0.356  0.077
Andujar      0.330  0.044
Smith        0.369  0.101
Simmons      0.340  0.061
Machado      0.367  0.113
Peralta      0.353  0.083
Dickerson    0.326  0.041
Duffy        0.360  0.091
Arenado      0.374  0.128
Goldschmidt  0.393  0.155
Baez         0.327  0.047
Peraza       0.328  0.047
Gurriel      0.323  0.044
Story        0.344  0.075
Rosario      0.323  0.054
Bregman      0.395  0.160
Blackmon     0.356  0.095
Bogaerts     0.357  0.105
Benintendi   0.363  0.126
Haniger      0.367  0.118
Suarez       0.369  0.124
Kemp         0.333  0.079
Votto        0.418  0.216
Lindor       0.355  0.106
Realmuto     0.345  0.081
Jones        0.310  0.041
Chapman      0.357  0.105
LeMahieu     0.326  0.071
Castro       0.329  0.081
Rizzo        0.375  0.126
Ozuna        0.321  0.063
Mauer        0.344  0.103
Marte        0.321  0.054
Ramirez      0.392  0.186

>>> y = batting_leaders_df[['AVG']]
>>> y
               AVG
Player
Betts        0.343
Martinez     0.328
Yelich       0.321
Altuve       0.317
Gennett      0.315
Trout        0.314
Cain         0.312
Zobrist      0.311
Freeman      0.311
Brantley     0.309
Rendon       0.308
Martinez     0.305
Merrifield   0.304
Segura       0.303
Castellanos  0.302
Markakis     0.301
Wendle       0.301
Andujar      0.298
Smith        0.297
Simmons      0.296
Machado      0.296
Peralta      0.296
Dickerson    0.296
Duffy        0.294
Arenado      0.294
Goldschmidt  0.293
Baez         0.292
Peraza       0.290
Gurriel      0.289
Story        0.289
Rosario      0.288
Bregman      0.288
Blackmon     0.287
Bogaerts     0.286
Benintendi   0.285
Haniger      0.285
Suarez       0.284
Kemp         0.284
Votto        0.284
Lindor       0.281
Realmuto     0.281
Jones        0.280
Chapman      0.280
LeMahieu     0.279
Castro       0.278
Rizzo        0.278
Ozuna        0.276
Mauer        0.276
Marte        0.276
Ramirez      0.274


>>> from sklearn.linear_model import LinearRegression
>>> regression = LinearRegression()
>>> model = regression.fit(X, y)

>>> redsox_batting_df = pd.read_table('redsox-batting-092518.txt')
>>> redsox_batting_df = redsox_batting_df.set_index(redsox_batting_df['Player'])
                  Player Team Pos    G   AB    R    H  2B  3B  ...    RBI  BB   SO  SB  CS    AVG    OBP    SLG    OPS
Player                                                         ...
Betts              Betts  BOS  RF  133  513  125  176  46   5  ...     80  78   90  29   6  0.343  0.434  0.639  1.073
Martinez        Martinez  BOS  LF  146  555  106  182  36   2  ...    124  69  143   6   1  0.328  0.402  0.622  1.023
Bogaerts        Bogaerts  BOS  SS  133  503   68  144  43   3  ...     96  53  100   8   2  0.286  0.357  0.509  0.866
Benintendi    Benintendi  BOS  LF  145  565  100  161  39   6  ...     84  71  104  21   3  0.285  0.363  0.460  0.823
Nunez              Nunez  BOS  2B  125  473   56  125  23   3  ...     44  15   69   7   2  0.264  0.288  0.389  0.677
Bradley Jr.  Bradley Jr.  BOS  CF  141  462   74  107  31   4  ...     58  44  135  16   1  0.232  0.311  0.400  0.711

[6 rows x 19 columns]

>>> test_set_X = redsox_batting_df[['OBP']]
>>> test_set_X = test_set_X.set_index(redsox_batting_df['Player'])
               OBP
Player
Betts        0.434
Martinez     0.402
Bogaerts     0.357
Benintendi   0.363
Nunez        0.288
Bradley Jr.  0.311

>>> test_set_y = redsox_batting_df[['AVG']]
>>> test_set_y
               AVG
Player
Betts        0.343
Martinez     0.328
Bogaerts     0.286
Benintendi   0.285
Nunez        0.264
Bradley Jr.  0.232

training set, fit model, test set

>>> p = model.predict(test_set_X)
>>> [ print(p[idx]) for idx in range(len(p)) ]
[0.34043838]
[0.32350771]
[0.28727735]
[0.28084986]
[0.25846742]
[0.24449761]

>>> [ print(" ", p[idx][0], test_set_y.iloc[idx][0], (p[idx][0] - test_set_y.iloc[idx][0])) 
          for idx in range(len(test_set_y)) ]
  0.34043838182134584 0.34299999999999997 -0.0025616181786541348
  0.32350771412287177 0.32799999999999996 -0.004492285877128188
  0.2872773485410336 0.28600000000000003 0.0012773485410335628
  0.28084986480512175 0.285 -0.004150135194878224
  0.25846742470403167 0.264 -0.005532575295968345
  0.24449760808234283 0.23199999999999998 0.012497608082342848
  













